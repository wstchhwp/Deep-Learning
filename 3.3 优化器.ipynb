{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer:  \n",
    "tf.train.GradientDescentOptimizer()    \n",
    "tf.train.AdadeltaOptimizer()    \n",
    "tf.train.AdagradOptimizer()     \n",
    "tf.train.AdagradDAOptimizer()    \n",
    "tf.train.MomentumOptimizer()   \n",
    "tf.train.AdamOptimizer()   \n",
    "tf.FtrlOptimizer()   \n",
    "tf.train.ProximalGradientDescentOptimizer()     \n",
    "tf.train.ProximalAdagradOptimizer()    \n",
    "tf.train.RMSPropOptimizer()    \n",
    "  \n",
    "# 各种优化器对比：\n",
    "标准梯度下降法：标准梯度下降先计算所有样本汇总误差，然后根据总误差来更新权值。  \n",
    "随机梯度下降法：随机梯度下降随机抽取一个样本来计算误差，然后更新权值  \n",
    "批量梯度下降法：批量梯度下降算是一种折中的方案，从总样本中选取一个批次（比如一共有1000的样本，随机选取100个样本作为一个batch）,然后计算这个batch的总误差，根据总误差来更新权值。   \n",
    "SGD（容易陷入局部最优，收敛速度很慢）  \n",
    "Momentum  \n",
    "NAG  \n",
    "Adagrad  \n",
    "RMSprop  \n",
    "Adadelta（建议使用，模型收敛速度快）  \n",
    "Adam  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.952,Learning Rate 0.001\n",
      "Iter 1,Testing Accuracy 0.9632,Learning Rate 0.00095\n",
      "Iter 2,Testing Accuracy 0.9666,Learning Rate 0.0009025\n",
      "Iter 3,Testing Accuracy 0.9735,Learning Rate 0.000857375\n",
      "Iter 4,Testing Accuracy 0.9738,Learning Rate 0.00081450626\n",
      "Iter 5,Testing Accuracy 0.9742,Learning Rate 0.0007737809\n",
      "Iter 6,Testing Accuracy 0.9757,Learning Rate 0.0007350919\n",
      "Iter 7,Testing Accuracy 0.975,Learning Rate 0.0006983373\n",
      "Iter 8,Testing Accuracy 0.9785,Learning Rate 0.0006634204\n",
      "Iter 9,Testing Accuracy 0.9773,Learning Rate 0.0006302494\n",
      "Iter 10,Testing Accuracy 0.9775,Learning Rate 0.0005987369\n",
      "Iter 11,Testing Accuracy 0.9777,Learning Rate 0.0005688001\n",
      "Iter 12,Testing Accuracy 0.9796,Learning Rate 0.0005403601\n",
      "Iter 13,Testing Accuracy 0.9796,Learning Rate 0.0005133421\n",
      "Iter 14,Testing Accuracy 0.9783,Learning Rate 0.000487675\n",
      "Iter 15,Testing Accuracy 0.979,Learning Rate 0.00046329122\n",
      "Iter 16,Testing Accuracy 0.9794,Learning Rate 0.00044012666\n",
      "Iter 17,Testing Accuracy 0.9779,Learning Rate 0.00041812033\n",
      "Iter 18,Testing Accuracy 0.9811,Learning Rate 0.00039721432\n",
      "Iter 19,Testing Accuracy 0.9803,Learning Rate 0.0003773536\n",
      "Iter 20,Testing Accuracy 0.9818,Learning Rate 0.00035848594\n",
      "Iter 21,Testing Accuracy 0.9805,Learning Rate 0.00034056162\n",
      "Iter 22,Testing Accuracy 0.9793,Learning Rate 0.00032353355\n",
      "Iter 23,Testing Accuracy 0.9807,Learning Rate 0.00030735688\n",
      "Iter 24,Testing Accuracy 0.9805,Learning Rate 0.000291989\n",
      "Iter 25,Testing Accuracy 0.9795,Learning Rate 0.00027738957\n",
      "Iter 26,Testing Accuracy 0.9806,Learning Rate 0.0002635201\n",
      "Iter 27,Testing Accuracy 0.9808,Learning Rate 0.00025034408\n",
      "Iter 28,Testing Accuracy 0.981,Learning Rate 0.00023782688\n",
      "Iter 29,Testing Accuracy 0.9799,Learning Rate 0.00022593554\n",
      "Iter 30,Testing Accuracy 0.9811,Learning Rate 0.00021463877\n",
      "Iter 31,Testing Accuracy 0.9807,Learning Rate 0.00020390682\n",
      "Iter 32,Testing Accuracy 0.9817,Learning Rate 0.00019371149\n",
      "Iter 33,Testing Accuracy 0.9821,Learning Rate 0.0001840259\n",
      "Iter 34,Testing Accuracy 0.9816,Learning Rate 0.00017482461\n",
      "Iter 35,Testing Accuracy 0.9807,Learning Rate 0.00016608338\n",
      "Iter 36,Testing Accuracy 0.9808,Learning Rate 0.00015777921\n",
      "Iter 37,Testing Accuracy 0.9813,Learning Rate 0.00014989026\n",
      "Iter 38,Testing Accuracy 0.9804,Learning Rate 0.00014239574\n",
      "Iter 39,Testing Accuracy 0.9804,Learning Rate 0.00013527596\n",
      "Iter 40,Testing Accuracy 0.9814,Learning Rate 0.00012851215\n",
      "Iter 41,Testing Accuracy 0.9813,Learning Rate 0.00012208655\n",
      "Iter 42,Testing Accuracy 0.9811,Learning Rate 0.00011598222\n",
      "Iter 43,Testing Accuracy 0.9815,Learning Rate 0.00011018311\n",
      "Iter 44,Testing Accuracy 0.9811,Learning Rate 0.000104673956\n",
      "Iter 45,Testing Accuracy 0.9809,Learning Rate 9.944026e-05\n",
      "Iter 46,Testing Accuracy 0.9812,Learning Rate 9.446825e-05\n",
      "Iter 47,Testing Accuracy 0.9811,Learning Rate 8.974483e-05\n",
      "Iter 48,Testing Accuracy 0.9821,Learning Rate 8.525759e-05\n",
      "Iter 49,Testing Accuracy 0.9824,Learning Rate 8.099471e-05\n",
      "Iter 50,Testing Accuracy 0.9817,Learning Rate 7.6944976e-05\n"
     ]
    }
   ],
   "source": [
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "# 每个批次的大小\n",
    "batch_size = 100  # 以矩阵的形式放进去\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001,dtype=tf.float32)\n",
    "\n",
    "# 创建一个复杂的神经网络\n",
    "W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))  # 截断的正态分布\n",
    "b1 = tf.Variable(tf.zeros([500])+0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))  # 截断的正态分布\n",
    "b2 = tf.Variable(tf.zeros([300])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1))  # 截断的正态分布\n",
    "b3 = tf.Variable(tf.zeros([10])+0.1)\n",
    "prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)\n",
    "\n",
    "\n",
    "# # 情形1：二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "# 情形2：对数似然代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "\n",
    "# 使用梯度下降算法\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))  # tf.argmax(prediction,1) 返回概率最大的位置（标签）\n",
    "# argmax返回一维张量中最大的值所在的位置\n",
    "\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(51):  # 迭代\n",
    "        sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "#             # Dropout防止过拟合\n",
    "#             sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
    "        learning_rate = sess.run(lr) \n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        print('Iter ' + str(epoch) + ',Testing Accuracy ' + str(test_acc)+ ',Learning Rate ' + str(learning_rate))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
